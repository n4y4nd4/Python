# -*- coding: utf-8 -*-
"""nayanda_robers_DR4_AT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16XUYDHkcsRjdDngAdARCDOuLbG59Adpv

1- Escreva um programa que leia o arquivo disponível em “https://github.com/cassiusf/datasets/blob/main/alunos_nomes.txt”. Armazene o conteúdo em um dicionário com pares chave-valor. Cada linha do arquivo deverá ser um par do dicionário com o número sendo a chave e o nome do(a) aluno(a) o valor. Após carregar o conteúdo em um dicionário, busque pelo seu nome e retorne o valor da chave correspondente.
"""

import requests
import pandas as pd
from io import StringIO

# URL do arquivo
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/alunos_nomes.txt"

# Baixar o conteúdo do arquivo
response = requests.get(url)

# Criar o DataFrame
df = pd.read_csv(StringIO(response.text), sep=" - ", header=None, names=['Numero', 'Nome'], engine='python')

# Converter o DataFrame em uma Série
serie = pd.Series(df['Numero'].values, index=df['Nome'])

# Converter a Série em um dicionário
dicionario = serie.to_dict()

nome = input("Digite o nome que você está procurando: ")

if nome in dicionario:
    numero = dicionario[nome]
    print(f"\nO número correspondente ao nome {nome} é {numero}.")
else:
    print(f"\nNome {nome} não encontrado.")

"""2- A partir do dicionário criado na questão 1, adicione o par contendo a chave 0 (zero) e o valor 'Cassius Figueiredo'. Depois de incluir esse par, salve seu dicionário em um arquivo TXT com o nome "novos_nomes.txt"."""

import requests
import pandas as pd
from io import StringIO

# URL do arquivo
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/alunos_nomes.txt"

# Baixar o conteúdo do arquivo
response = requests.get(url)

# Ler o arquivo e criar o DataFrame usando ' - ' como delimitador
df = pd.read_csv(StringIO(response.text), sep=" - ", header=None, names=['Numero', 'Nome'], engine='python')

# Converter o DataFrame em uma Série
serie = pd.Series(df['Numero'].values, index=df['Nome'])

# Converter a Série em um dicionário
dicionario = serie.to_dict()

# Adicionar o novo par chave-valor
dicionario['Cassius Figueiredo'] = '0'

# Nome do arquivo para salvar o dicionário
nome_arquivo = "novos_nomes.txt"

# Salvar o dicionário em um arquivo de texto
with open(nome_arquivo, 'w', encoding='utf-8') as f:
    for chave, valor in dicionario.items():
        f.write(f"{chave}: {valor}\n")

print(f"Dicionário salvo em '{nome_arquivo}'.")

print(f"Dicionário alterado:")
for chave, valor in dicionario.items():
    print(f"{chave}: {valor}")


# Verificar se o nome 'Cassius Figueiredo' foi incluído com sucesso
if 'Cassius Figueiredo' in dicionario.values():
    print("Nome 'Cassius Figueiredo' incluído com sucesso.")

df

"""
3- A partir do dicionário alterado na questão 2, crie dois conjuntos (sets), um apenas com as chaves e outro com os valores. Crie então um novo dicionário, utilizando os conjuntos, só que agora com as informações invertidas: os nomes serão as chaves e o número o valor.

"""

import requests
import pandas as pd
from io import StringIO

# URL do arquivo
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/alunos_nomes.txt"

# Baixar o conteúdo do arquivo
response = requests.get(url)

# Ler o arquivo e criar o DataFrame usando ' - ' como delimitador
df = pd.read_csv(StringIO(response.text), sep=" - ", header=None, names=['Numero', 'Nome'], engine='python')

# Converter o DataFrame em uma Série
serie = pd.Series(df['Numero'].values, index=df['Nome'])

# Converter a Série em um dicionário
dicionario = serie.to_dict()

# Adicionar o novo par chave-valor
dicionario['Cassius Figueiredo'] = '0'

# Dicionário original
print("Dicionário original:")
for chave, valor in dicionario.items():
    print(f"{chave}: {valor}")

# Criar novo dicionário invertendo as informações
novo_dicionario_invertido = {valor: chave for chave, valor in dicionario.items()}

# Exibir o novo dicionário invertido
print("\nNovo dicionário invertido:")
for chave, valor in novo_dicionario_invertido.items():
    print(f"{chave}: {valor}")

"""4- A partir do dicionário alterado na questão 2, busque por todos os alunos que têm seu nome iniciado pela letra "G" e elimine-os do dicionário. Armazene este novo dicionário em um arquivo texto chamado "novo_arquivo.txt"."""

import requests
import pandas as pd
from io import StringIO

# URL do arquivo
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/alunos_nomes.txt"

# Baixar o conteúdo do arquivo
response = requests.get(url)

# Ler o arquivo e criar o DataFrame usando ' - ' como delimitador
df = pd.read_csv(StringIO(response.text), sep=" - ", header=None, names=['Numero', 'Nome'], engine='python')

# Converter o DataFrame em uma Série
serie = pd.Series(df['Numero'].values, index=df['Nome'])

# Converter a Série em um dicionário
dicionario = serie.to_dict()

# Adicionar o novo par chave-valor
dicionario['Cassius Figueiredo'] = '0'

# Nome do arquivo para salvar o dicionário
nome_arquivo = "novos_nomes.txt"

# Buscar por alunos cujos nomes começam com "G" e eliminar do dicionário
alunos_g = [nome for nome in dicionario.keys() if nome.startswith('G')]
for aluno in alunos_g:
    del dicionario[aluno]

# Salvar o dicionário atualizado em um arquivo de texto
with open(nome_arquivo, 'w', encoding='utf-8') as f:
    for chave, valor in dicionario.items():
        f.write(f"{chave}: {valor}\n")

print(f"Dicionário alterado e salvo em '{nome_arquivo}':")
for chave, valor in dicionario.items():
    print(f"{chave}: {valor}")

"""5- Escreva um programa que leia o arquivo disponível em “https://raw.githubusercontent.com/cassiusf/datasets/main/penguins_cleaned.csv” (preferencialmente, diretamente do Github). Armazene o conteúdo em um dataframe e apresente-o no Colab."""

import pandas as pd
import requests
from io import StringIO

# URL do arquivo CSV no GitHub
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/penguins_cleaned.csv"

# Fazer o download do conteúdo do arquivo CSV
response = requests.get(url)

# Ler o conteúdo do arquivo CSV em um DataFrame
df = pd.read_csv(StringIO(response.text))

# Exibir o DataFrame para verificar o conteúdo
print("Conteúdo do DataFrame:")
print(df)

"""6- A partir do dataframe carregado na questão 5, apresente a média das variáveis "bill_length_mm", "bill_depth_mm" e "flipper_length_mm" por ilha e gênero."""

import pandas as pd
import requests
from io import StringIO

# URL do arquivo CSV no GitHub
url = "https://raw.githubusercontent.com/cassiusf/datasets/main/penguins_cleaned.csv"

# Fazer o download do conteúdo do arquivo CSV
response = requests.get(url)

# Ler o conteúdo do arquivo CSV em um DataFrame
df = pd.read_csv(StringIO(response.text))

# Exibir o DataFrame para verificar o conteúdo
print("Conteúdo do DataFrame:")
print(df)

# Calcular a média das variáveis por ilha e gênero
medias = df.groupby(['island', 'sex'])[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']].mean()

# Exibir as médias calculadas
print("\nMédia das variáveis por ilha e gênero:")
print(medias)

"""7- Carregue o arquivo em formato JSON a partir do link “https://data.nasa.gov/resource/y77d-th95.json” para um dataframe Apresente seu conteúdo no Colab."""

import pandas as pd
import requests

# URL do arquivo JSON
url = "https://data.nasa.gov/resource/y77d-th95.json"

# Fazer o download do conteúdo do arquivo JSON
response = requests.get(url)


# Carregar o conteúdo JSON em um DataFrame
df = pd.read_json(response.text)

# Exibir o DataFrame para verificar o conteúdo
print("Conteúdo do DataFrame:")
print(df)

"""8- A partir da base carregada na questão 7, apresente a quantidade de classes únicas da base (variável "recclass") e a média da variável "mass" para cada uma das classes da variável "fall"."""

import pandas as pd
import requests

# URL do arquivo JSON
url = "https://data.nasa.gov/resource/y77d-th95.json"

# Fazer o download do conteúdo do arquivo JSON
response = requests.get(url)


# Carregar o conteúdo JSON em um DataFrame
df = pd.read_json(response.text)

# Exibir o DataFrame
print("Conteúdo do DataFrame:")
print(df)

# Contagem de classes únicas da variável "recclass"
num_classes_unicas = df['recclass'].nunique()
print(f"Número de classes únicas da variável 'recclass': {num_classes_unicas}")

# Média da variável "mass" para cada classe da variável "fall"
media_mass_por_fall = df.groupby('fall')['mass'].mean()
print("\nMédia da variável 'mass' para cada classe da variável 'fall':")
print(media_mass_por_fall)

"""9- Faça o download do arquivo disponível em “https://github.com/cassiusf/datasets/blob/main/diamonds.xlsx” (preferencialmente, diretamente do Github). Armazene o conteúdo em um dataframe e apresente-o no Colab."""

import pandas as pd

# URL do arquivo Excel no GitHub
url = "https://github.com/cassiusf/datasets/raw/main/diamonds.xlsx"

# Carregar o arquivo Excel em um DataFrame
df = pd.read_excel(url, engine='openpyxl')

# Exibir o DataFrame para verificar o conteúdo
print("Conteúdo do DataFrame:")
print(df)

"""10- A partir da base carregada na questão 9, apresente as médias de preço e quilates ("carat") e a mediana da variável "depth" para os diamantes da base, agrupando-os pelas variáveis "cut" e "color"."""

import pandas as pd

# URL do arquivo Excel no GitHub
url = "https://github.com/cassiusf/datasets/raw/main/diamonds.xlsx"

# Carregar o arquivo Excel em um DataFrame
df = pd.read_excel(url, engine='openpyxl')

# Calcular médias de preço e quilates e mediana de depth agrupando por cut e color
calcular = df.groupby(['cut', 'color']).agg({'price': 'mean', 'carat': 'mean', 'depth': 'median'})

# Exibir o resumo
print("Médias de preço e quilates e mediana de depth por Cut e Color:")
print(calcular)

"""11- Carregue a base em SQLite que está em “https://github.com/cassiusf/datasets/blob/main/mpg.db” para um dataframe. Faça as conversões de tipo necessárias para ajustar as variáveis aos dados de forma correta."""

import sqlite3
import pandas as pd
import requests
import tempfile

# URL do arquivo SQLite
url = "https://github.com/cassiusf/datasets/blob/main/mpg.db?raw=true"

# Fazer o download do conteúdo do arquivo SQLite
response = requests.get(url)

# Criar um arquivo temporário
with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
    tmp_file.write(response.content)
    tmp_file_path = tmp_file.name

# Conectar ao banco de dados SQLite usando o caminho do arquivo temporário
conn = sqlite3.connect(tmp_file_path)

# Verificar quais tabelas estão no banco de dados
tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn)
print("Tabelas no banco de dados:", tables)

# Carregar as tabelas para DataFrames e exibir as primeiras linhas
df_observation = pd.read_sql_query("SELECT * FROM Observation LIMIT 5", conn)
df_origin = pd.read_sql_query("SELECT * FROM Origin LIMIT 5", conn)
df_name = pd.read_sql_query("SELECT * FROM Name LIMIT 5", conn)

print("Primeiras linhas da tabela Observation:")
print(df_observation)

print("Primeiras linhas da tabela Origin:")
print(df_origin)

print("Primeiras linhas da tabela Name:")
print(df_name)

# Fechar a conexão
conn.close()

"""12- A partir da base carregada na questão 11, apresente as médias de consumo ("mpg"), potência ("horsepower") e peso ("weight") por origem e quantidade de cilindros. Na mesma tabela final, apresente também a quantidade de carros para cada ano existente na base, a partir da variável "model_year"."""

import sqlite3
import pandas as pd
import requests
import tempfile

# URL do arquivo SQLite
url = "https://github.com/cassiusf/datasets/blob/main/mpg.db?raw=true"

# Fazer o download do conteúdo do arquivo SQLite
response = requests.get(url)

# Criar um arquivo temporário
with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
    tmp_file.write(response.content)
    tmp_file_path = tmp_file.name

# Conectar ao banco de dados SQLite usando o caminho do arquivo temporário
conn = sqlite3.connect(tmp_file_path)

# Carregar as tabelas para DataFrames
df_observation = pd.read_sql_query("SELECT * FROM Observation", conn)
df_origin = pd.read_sql_query("SELECT * FROM Origin", conn)
df_name = pd.read_sql_query("SELECT * FROM Name", conn)

# Fechar a conexão
conn.close()

# Juntar a tabela Observation com Origin para obter os nomes das origens
df_merged = df_observation.merge(df_origin, on='origin_id', suffixes=('', '_origin'))

# Calcular as médias de 'mpg', 'horsepower' e 'weight' por 'origin' e 'cylinders'
mean_values = df_merged.groupby(['origin', 'cylinders']).agg({
    'mpg': 'mean',
    'horsepower': 'mean',
    'weight': 'mean'
}).reset_index()

# Contar a quantidade de carros para cada ano na variável 'model_year'
car_count_by_year = df_merged.groupby('model_year').size().reset_index(name='car_count')

# Exibir os resultados
print("Médias de consumo (mpg), potência (horsepower) e peso (weight) por origem e quantidade de cilindros:")
print(mean_values)

print("\nQuantidade de carros por ano:")
print(car_count_by_year)

"""13- Faça um código em Python que tente carregar a base disponível em “https://github.com/cassiusf/datasets/blob/main/films.json” para um dataframe. Implemente uma estrutura completa de tratamento de erros, utilizando a estrutura "try...except...else...finally" que verifique se o arquivo está acessível e, caso não esteja, emita uma mensagem informando o usuário do problema. Não esqueça de implementar de forma coerente as estruturas "else..." e "finally..."."""

import pandas as pd
import requests

# URL do arquivo JSON
url = "https://github.com/cassiusf/datasets/raw/main/films.json"

try:
    # Tentar baixar o conteúdo do arquivo JSON
    response = requests.get(url)

    # Verificar se a resposta foi bem sucedida (código 200)
    if response.status_code == 200:
        # Tentar carregar o conteúdo JSON em um DataFrame
        try:
            df = pd.read_json(response.text)
        except ValueError as ve:
            # Tratar erro se não conseguir ler o JSON corretamente
            print(f"Erro ao ler o JSON: {ve}")
        else:
            # Se conseguiu carregar sem erros, exibir algumas linhas do DataFrame
            print("DataFrame carregado com sucesso:")
            print(df)
        finally:
            # Fechar a conexão ou liberar recursos se necessário
            response.close()
    else:
        # Emitir mensagem se o servidor retornar código de erro
        print(f"Erro ao acessar o arquivo: Status {response.status_code}")
except requests.RequestException as e:
    # Tratar erro de conexão
    print(f"Erro de conexão: {e}")
finally:
    # Outras ações de limpeza ou encerramento
    print("Fim do processo.")

"""14- Escolha uma página qualquer da Wikipedia em português e apresente a página escolhida. Implemente em linguagem Python uma raspagem de dados do site da Wikipedia, na página escolhida utilizando as bibliotecas requests e BeautifulSoup. Armazene o resultado da raspagem inicial em um objeto (a chamada "sopa") e apresente este objeto em seu Colab."""

import requests
from bs4 import BeautifulSoup

# URL da página da Wikipedia
url = "https://pt.wikipedia.org/wiki/One_Direction"


# Fazer a requisição GET para obter o conteúdo da página
response = requests.get(url)

# Criar o objeto BeautifulSoup para analisar o conteúdo HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Exibir o objeto soup para visualizar a estrutura da página
print(soup)

"""15- Capture e armazene em um dataframe todos os links existentes na seção "Referências" da página escolhida e apresente-o. Você deverá utilizar a "sopa" criada na questão anterior."""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL da página da Wikipedia
url = "https://pt.wikipedia.org/wiki/One_Direction"

# Fazer a requisição GET para obter o conteúdo da página
response = requests.get(url)

soup = BeautifulSoup(response.content, 'html.parser')

# Encontrar a seção de referências na página
references_section = soup.find('ol', class_='references')

# Encontrar todos os links (tags <a>) dentro da seção de referências
links = references_section.find_all('a', class_='external text', href=True)

# Armazenar os links em uma lista de dicionários
data = [{'URL': link['href']} for link in links]

# Criar o DataFrame a partir da lista de dicionários
df_links = pd.DataFrame(data)

# Exibir o dataframe com os links das referências
print("Links das Referências:")
print(df_links)

"""16- Escolha uma outra seção da página (diferente da "Referências") e execute o mesmo processo da questão anterior, apresentando seus resultados de forma organizada no Colab (não precisa usar o método "prettify()")."""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL da página da Wikipedia
url = "https://pt.wikipedia.org/wiki/One_Direction"

# Fazer a requisição GET para obter o conteúdo da página
response = requests.get(url)

# Criar o objeto BeautifulSoup para analisar o conteúdo HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Encontrar a seção de "Ligações externas" na página
ligacoes_externas_section = soup.find('span', id='Ligações_externas').parent

# Verificar se a seção foi encontrada
if ligacoes_externas_section:
    # Encontrar a lista não ordenada (ul) que segue a seção "Ligações externas"
    ligacoes_externas_list = ligacoes_externas_section.find_next('ul')

    # Encontrar todos os links (tags <a>) dentro da lista de "Ligações externas"
    links = ligacoes_externas_list.find_all('a', href=True)

    # Armazenar os links em uma lista de dicionários
    data = [{'Texto': link.text, 'URL': link['href']} for link in links]

    # Criar um DataFrame com os links da seção "Ligações externas"
    df_links_ligacoes_externas = pd.DataFrame(data)

    # Exibir o DataFrame com os links da seção "Ligações externas"
    print("Links da seção 'Ligações externas':")
    print(df_links_ligacoes_externas)
else:
    print("Seção de 'Ligações externas' não encontrada na página.")

"""17- Execute o mesmo processo das questões anteriores, porém agora capturando apenas os links existentes após o texto "Ver artigo principal:" da página escolhida. Apresente todos os links capturados no Colab.

"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL da página da Wikipedia
url = "https://pt.wikipedia.org/wiki/One_Direction"

# Fazer a requisição GET para obter o conteúdo da página
response = requests.get(url)

# Criar o objeto BeautifulSoup para analisar o conteúdo HTML
soup = BeautifulSoup(response.content, 'html.parser')

# Encontrar todos os elementos que podem conter "Ver artigo principal:"
hatnotes = soup.find_all('div', class_='hatnote')

# Inicializar uma lista para armazenar os links
data = []

# Iterar sobre os elementos encontrados
for hatnote in hatnotes:
    if "Ver artigo principal:" in hatnote.text:
        # Encontrar todos os links (tags <a>) dentro do elemento atual
        links = hatnote.find_all('a', href=True)
        # Armazenar os links em uma lista de dicionários
        data.extend([{'Texto': link.text, 'URL': link['href']} for link in links])

# Criar um DataFrame com os links encontrados
df_links_ver_artigo_principal = pd.DataFrame(data)

# Exibir o DataFrame com os links encontrados
print("Links após 'Ver artigo principal':")
print(df_links_ver_artigo_principal)